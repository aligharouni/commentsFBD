---
title: "Fréchet distance between curves and curve-based descriptive statistics"
author: "Ali Gharouni and Ben Bolker"
date: "`r format(Sys.time(),'%d %b %Y')`"
output: html_document
bibliography: ../AliMac.bib
---

```{r setup, include=FALSE}
## https://stackoverflow.com/questions/55009313/how-to-use-cairo-pngs-in-r-markdown
## requires Cairo package ...
knitr::opts_chunk$set(echo = TRUE, dev.args = list(png = list(type = "cairo")))
knitr::knit_hooks$set(basefig=function(before, options, envir) {
                   if (before) {
                       par(bty="l",las=1)
                   } else { }
})

```

## Questions

0. Can we get a clearer description of how Juul's methods work/how they would fit into other definitions of centrality? (randomly sample a subset of curves; compute the min-max envelopes; score individual curves based on whether they lie entirely within the envelope or not; repeat to derive a centrality score on [0,1] for each curve based on the proportion)
1. Do we apply measures of centrality directly to the curves or to probes derived from the curves?
2. What measure of centrality/typical-ness do we apply?
3. If it depends on distance, what distance do we use?
4. Do we define centrality by difference from a central point (centroid/Fréchet mean) or by average closeness to the rest of the set? Does the answer differ depending on step 3 (choice of distance)?

```{r graph_setup, echo=FALSE}
g_labels <- c("trajectories", "fixed-time mean",
            "fixed-time median", "Fréchet mean")
g_cols <- c("gray","blue","purple","red")
```

```{r packages, echo=FALSE, message=FALSE}
library(SimilarityMeasures) ## for Fréchet distance computation
source('functions.R')
```

This is inspired by @juul2021fixed. 

In the paper, pitfalls of fixed-time mean trajectory (the pointwise mean at each time step) are explored and an alternative curve-based method, which is based on ranking the curves and sampling, is studied.

Juul *et al* suggest three different ways to rank the centrality of curves;

- all-or-nothing ranking method (Fig.2b,c)
- weighted ranking (Fig.2d)
- according to some feature of interest (Fig.2e), e.g. the projected peak value

The first two approaches, which involve subsampling, seem odd to me (BMB). Is the sampling-based approach done to minimize computational cost? The approach suggested below is computationally expensive, since it requires computing *all* pairwise distances between the curves in the ensemble (there may be a clever way to do this, although a quick lit search gives only metric-specific algorithms [e.g. on the space of phylogenetic trees or on the hypersphere], not general ideas).

Our approach is to use the Fréchet distance (see below) and quantiles of centrality.
  
- suppose we have a set of trajectories.
- Fréchet distance between the trajectories to be calculated,
- the distribution of the distances can be estimated, (**BMB**: what does this tell us?)
- the curve-based descriptive statistics can be [?] as follows 
  (1) ranking the curves from more central (closer to the distribution mean/median I propose) to less central
  (2) Plot the envelope containing the most central curves, i.e. the pointwise min/max curves of a subset of curves with the lowest sum (or sum of squares) of distances to the other curves.

## Fréchet distances

The [Fréchet distance](https://en.wikipedia.org/wiki/Fr%C3%A9chet_distance) is a distance metric for curves.

```{r simple1, echo= FALSE, results="hide"}
# Creating two trajectories where the first col is time steps
path1 <- matrix(c(0, 1, 2, 3, 0, 1, 2, 3), 4)
path2 <- matrix(c(0, 1, 2, 3, 4, 5, 6, 7), 4)
# Running the Fréchet distance algorithm.
Frechet(path1, path2)
# the same as above
Frechet(matrix(path1[,2]),matrix(path2[,2]))
```

```{r simple2}
n_sample <- 10 ## num of data points, days
n_traj <- 20 ## number of trajectories
t <- 1:n_sample
set.seed(1234)
M <- replicate(n_traj,rnorm(n_sample,mean = n_sample/2))
```

```{r fixedtime1, fig.cap="Fixed-time mean/median; the pointwise mean at each time step",basefig=TRUE}
matplot(x=t,y=M,type = "l", lty=1, col = g_cols[1], ylab="")
lines(t,rowMeans(M), col = g_cols[2], lwd = 2)
lines(t, apply(M,1,median),col=g_cols[3])
legend("topright", legend = g_labels[1:3] , col=g_cols[1:3],
       lty=1, bg="white")       
```

Calculate all of the pairwise Fréchet distances between trajectories:

```{r frechet_dist}
dmat <- matrix(NA, n_traj, n_traj)
diag(dmat) <- 0
for (i in 1:(n_traj-1)) {
    for (j in (i+1):n_traj) {
        dmat[i,j] <- Frechet(matrix(M[,i]),matrix(M[,j]))
    }
}

dmat <- make_symm(dmat)
stopifnot(isSymmetric(dmat))
```

# histogram of Fréchet dist
```{r fig.cap="density plot of Fréchet distances"}
hist(dmat[lower.tri(dmat)],xlab="Fréchet distance",main="",breaks=20)
```

Find the [Fréchet mean](https://en.wikipedia.org/wiki/Fr%C3%A9chet_mean) (min average distance = most central point), considering the ensemble of curves to be the entire set of possibilities

```{r frechet2}
## Fréchet mean corresponding to arithmetic mean: minimize sum of squares of distances
md2 <- rowSums(dmat^2)  
fm2 <- which.min(md2)
## Fm corresponding to median: minimize sum of distances 
md1 <- rowSums(dmat)
fm1 <- which.min(md1)
```

In this case the Fréchet mean & median are the same;
but the rankings for (sum of distances to all other curves) and (sum of squared distances) are not quite identical, so regions will differ. We'll use the 'median-like' criterion (sum of distances).

```{r fcor}
cor.test(md1,md2,method="spearman")$estimate
```

Find the envelope of trajectories with the most central 50% (is this right??)

```{r find_envelope}
central_curves <- which(md1<quantile(md1,0.5))
## POINTWISE min/max of these curves (?)
fmin <- apply(M[,central_curves],1,min)
fmax <- apply(M[,central_curves],1,max)
```

Compute pointwise central 50% quantiles for comparison:
```{r pointquant}
pqmin <- apply(M,1,quantile,0.25)
pqmax <- apply(M,1,quantile,0.75)
```

```{r plot_all,basefig=TRUE,fig.cap="fixed-time and Fréchet metrics: gray (wide)=50% central Fréchet region, green (narrow)=fixed-time central region"}
matplot(x=t,y=M,type = "l", col = g_cols[1], lty=1,ylim=c(0,10))
lines(t,rowMeans(M), col = g_cols[2], lwd = 2)
lines(t,apply(M,1,median), col = g_cols[3], lwd = 2)
lines(t,M[,fm1], col = g_cols[4], lwd = 2)
polygon(c(t,rev(t)),c(fmin,rev(fmax)), col=adjustcolor("black",alpha.f=0.2),
        border=NA)
polygon(c(t,rev(t)),c(pqmin,rev(pqmax)), col=adjustcolor("green",alpha.f=0.2),
        border=NA)
legend("topright", legend = g_labels,
       col= g_cols, lty=1, bg="white")       
```

---

The region here (which is supposed to represent the central 50% of the sample) seems a bit odd; the Fréchet envelope (in black) is much wider than the pointwise 50% range (in green). Maybe that's as expected?

The Fréchet distance is a natural choice given that we are working with curves, but there's no reason that we have to use that distance metric: the Fréchet **mean** (and associated concepts of centrality) is (IMO) more important than the choice of distance metric. For example, we could use a Mahalanobis distance on the time points (although this doesn't include any information, e.g., about the time-ordering of the points, which seems weird?), or the Mahalanobis distance of a set of epidemiologically relevant metrics (e.g. {peak time, peak height, time to 5% of cumulative infection, epidemic duration}).

It would be interesting to apply this approach to @juul2021fixed's examples. Or we could apply Juul *et al.*'s algorithms to this example.

- Does our approach have any advantages other than being (apparently) better posed?
- Does @juul2021fixed's sampling-based approach have any theoretical justification? Does it correspond to some kind of approximation of central components as defined here? (The distance metric is certainly different ...)

More searching for prior art: does anyone in the functional data analysis world do anything like this already? Where would we look/who would we ask? ("Fréchet" is a nice distinctive search term, but so far I have found anything directly relevant ...)

AGh:[Virginia Vassilevska Williams](https://scholar.google.com/citations?hl=en&user=Wph6P_IAAAAJ&view_op=list_works&sortby=pubdate) has worked on the sequence walk problem. She has a youtube [talk](https://www.youtube.com/watch?v=XCoCiXIShyk). 
For theoretical stuff see @abboud2015tight

**BMB: I'm not sure this is relevant?**

## Playing with norms

```{r n1}
tvec <- seq(0,10,length=101)
y1 <- ifelse(tvec<5,0,1)
y2 <- ifelse(tvec<5.5,0,1)
matplot(tvec,cbind(y1,y2),type="l")
max(abs(y1-y2))
Frechet(matrix(y1),matrix(y2))
DTW(matrix(y1),matrix(y2))
```

Consider distances via [dynamic time warping](https://en.wikipedia.org/wiki/Dynamic_time_warping), which is also implemented in `SimilarityMeasures` and might be slightly more quantitative/less "topological" than Fréchet distances? (Although Mahalanobis on probes/features also seems to make a lot of sense.)

Note that in the example above, n1, any horizontal shift of curve, results in 0 Fréchet distance. 

## Mahalanobis norm 

Mahalanobis norm can be used to determine whether a sample is an outlier, whether a process is in control or whether a sample is a member of a group or not (see [Richard G. Brereto's paper](https://onlinelibrary.wiley.com/doi/full/10.1002/cem.2692)).

The squared Mahalanobis norm of $y$ for a given center $c$ is `(y-c) %*% sy^-1 %*% t((y-c))`

```{r mahalanobis_simple_example}
## num of trajs > num of points to get cov 
n_traj0 <- 3 ## 3 trajectories, 2 data points
n_point0 <- 2
traj <- matrix(NA,n_traj0,n_point0) ## trajectories are in rows
traj[1,] <- c(0,0)
traj[2,] <- c(1,1)
traj[3,] <- c(0,1)
Sigma0 <- cov(traj) ## 2by2

matplot(t(traj),type="b")
## we can take the whole traj matrix
mahalanobis(traj,center = colMeans(traj),cov=Sigma0) ##  all identical, 

## pairwise
mahalmat0 <- matrix(NA, n_traj0, n_traj0)
diag(mahalmat0) <- 0

for (i in 1:(n_traj0-1)) {
    for (j in (i+1):n_traj0) {
        ## calculating pairwise M distance of both trajectories from their mean
        ## (identical, take the first one)
        mahalmat0[i,j] <- mahalanobis(traj[c(i,j),], #taj i and j
                                     center=colMeans(traj[c(i,j),]),
                                     cov=Sigma0)[1]
    }
}
mahalmat0 <- make_symm(mahalmat0)

```

Question: what do we mean by "Mahalanobis on probes/features"?
Answer: pick a set of values ("summary statistics", "epidemiologically relevant metrics"): these could be min/max/mean/median of incidence, duration, growth rate, total number infected, ...

```{r mahalanobis_probes1}
## mahalanobis on the features, make sure the # of trajs > # of probes
## traj is taken from above
n_probes0 <- 2
probes0 <- matrix(NA,n_traj0,n_probes0) 
probes0[,1] <- apply(traj, 1, FUN = min)
probes0[,2] <- apply(traj, 1, FUN = max)   

Sigma1 <- cov(probes0) ## 2by2
## pairwise
mahalmat0 <- matrix(NA, n_traj0, n_traj0)
diag(mahalmat0) <- 0

for (i in 1:(n_traj0-1)) {
    for (j in (i+1):n_traj0) {
        ## calculating pairwise M distance of both trajectories from their mean
        ## (identical, take the first one)
        mahalmat0[i,j] <- mahalanobis(probes0[c(i,j),], #taj i and j
                                      center=colMeans(probes0[c(i,j),]),
                                      cov=Sigma0)[1]
    }
}
mahalmat0 <- make_symm(mahalmat0)

```


It's harder to think about computing the *pairwise* Mahalanobis distance between the features of two curves, because we can't compute the covariance matrix for only two samples, but we can still use the covariance matrix from the entire data set as the denominator/scaling factor (this might get weird if the distribution of probes is weird, e.g. multimodal, but still usable?)

Use `M` from above so we can compute $\Sigma$ (i.e. number of curves > number of time points): here we're using the actual trajectory values, *not* summary statistics:

```{r mahal2}
Sigma <- cov(t(M)) ## covariance matrix of points (time)
mahalmat <- matrix(NA, n_traj, n_traj)
diag(mahalmat) <- 0
for (i in 1:(n_traj-1)) {
    for (j in (i+1):n_traj) {
        ## calculating pairwise M distance of both trajectories from their mean
        ## (identical, take the first one)
        mahalmat[i,j] <- mahalanobis(t(M[,c(i,j)]), #taj i and j
                                     center=rowMeans(M[,c(i,j)]),
                                     cov=Sigma)[1]
    }
}
mahalmat <- make_symm(mahalmat)
```

```{r mahalanobis_probes2}
## using t(M), as I would like to put trajs on the rows
print(n_traj)
n_probes1 <- 4
probes1 <- matrix(NA,n_traj,n_probes1) 
probes1[,1] <- apply(t(M), 1, FUN = min)
probes1[,2] <- apply(t(M), 1, FUN = max)   
probes1[,3] <- apply(t(M), 1, FUN = median)   
probes1[,4] <- rowMeans(t(M))   

Sigma1 <- cov(probes1)
## pairwise
mahalmat1 <- matrix(NA, n_traj, n_traj) ## Mahalanobis
diag(mahalmat1) <- 0

frechet1 <- matrix(NA, n_traj, n_traj) ## Frechet
diag(frechet1) <- 0

for (i in 1:(n_traj-1)) {
    for (j in (i+1):n_traj) {
        ## calculating pairwise M distance of both trajectories from their mean
        ## (identical, take the first one)
        mahalmat1[i,j] <- mahalanobis(probes1[c(i,j),], #taj i and j
                                     center=colMeans(probes1[c(i,j),]),
                                     cov=Sigma1)[1]
        ## Frechet
        frechet1[i,j] <- Frechet(matrix(probes1[i,]),matrix(probes1[j,]))
    }
}
mahalmat1 <- make_symm(mahalmat1)
mahaldist2 <- mahalanobis(probes1,center=colMeans(probes1),cov=Sigma1)

frechet1 <- make_symm(frechet1)
frechet2 <- apply(probes1,1,FUN=function(x){Frechet(matrix(x),matrix(colMeans(probes1)))} )

```

```{r}
rank(rowMeans(mahalmat1))
rank(mahaldist2)

rank(rowMeans(frechet1))
rank(frechet2)
```


## Reference(s)



