\documentclass[12pt]{article}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage[nameinlink,capitalize]{cleveref}
\usepackage{cleveref}
\usepackage[margin=1in]{geometry}
\usepackage{lineno}\renewcommand\thelinenumber{\color{gray}\arabic{linenumber}}
\usepackage{pdflscape}
\usepackage{xspace}
\usepackage{array}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\comment}{\showcomment}
\newcommand{\showcomment}[3]{\textcolor{#1}{\textbf{[#2: }\textsl{#3}\textbf{]}}}
\newcommand{\nocomment}[3]{}

\newcommand{\ali}[1]{\comment{magenta}{Ali}{#1}}
\newcommand{\bmb}[1]{\comment{red}{BMB}{#1}}
\newcommand{\todo}[1]{\comment{red}{TODO}{#1}}

\theoremstyle{definition} % amsthm only
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
 
\bibliographystyle{apalike}

\title{Centrality: A Curve-Based Statistical Discription of an Ensemble}
\author{Ali Gharouni, Ben Bolker}
\begin{document}
\maketitle
\linenumbers

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Abstract}
%\bmb{revisit after revising body}
%This is a commentary work motivated by \cite{juul2021fixed}'s work in which a few useful ideas of the concept of the central set out of an ensemble of epidemic curves were presented. In the present work we provide alternative, and more principled, curved-based statistics approaches to approximate the most central set which represents the central 50\% of the ensemble. In particular, we use three functional ranking methods; (1) the sampling-based, fast and robust functional boxplot, 
%(2) pairwise distances (eg, $\ell_2$ norm) between the curves and quantiles of centrality, and (3) a multivariate generalization of ranking the curves by using Mahalanobis distance among features of interest. We apply our methods on \cite{juul2021fixed}'s dataset and compare our results with theirs.      
 
\section{Introduction}
%% However, the reason/potential advantage of aparting from the standard functional boxplot framework is unclear \ali{to be toned down a bit?!}.  

%% presented the following useful curve-based alternative methods to the fixed-time statistics of epidemic curve ensembles; (i) all-or-nothing ranking method (presented in their Fig.2b,c), (ii) weighted ranking (presented in their Fig.2d) and (iii) according to some feature of interest, e.g. the projected peak value (presented in their Fig.2e). Note that (i) and (ii) are sampling-based approaches and (iii) is a featur-based ranking approach. They cautioned that standard pointwise (fixed-time) averages may not be appropriate to summerize ensembles of epidemic curves. Particularly, they discussed that miscapturing key features of an epidemic such as the peak numbers of infections, the time of the peak, etc. may result in obscuring the forcast process. While Juul et al.'s implimented their methodology in sampling-based functional ranking and establishing the most ``central set'' of an ensemble, there is a deep existing literature in functional depth, functional boxplot, and centrality metrics for high dimension data \citep{fraiman2001trimmed, lopez2007depth, lopez2009concept, sun2011functional,sun2012exact}. It appears that \cite{juul2021fixed} were aware of the functional boxplot concept, since they cited the work by \cite{sun2011functional}. However, the reason/potential advantage of aparting from the standard functional boxplot framework is unclear \ali{to be toned down a bit?!}.  


%The idea of the functional boxplot goes back farther to the notion of depth. It was first introduced for multivariate data in an attempt to generalize the ideas of order statistics, ranks and medians into higher dimensions (see for e.g., \cite{mahalanobis1936generalized,tukey1975mathematics}). The notion of depth was extended for functional data by \citep{lopez2009concept}. They introduced the concept of band depth (BD) for ranking a sample of functional data from the center outward. This ordering enables us to define the functional quantiles, centrality or outlyingness of an observation. Further, the classical boxplot for univariate data was extended to the functional boxplots and adjusted functional boxplots as a visualization tool \citep{sun2011functional,sun2012adjusted}.

%We note that \cite{juul2021fixed}'s sampling-based approach is a special case of functional depth in which the robustness and optimized computational cost of the functional band depth is known \citep{sun2012exact}. The computational cost is determined by the sample size of the ensemble $n$ and the number of curves $j$ defining a band/envelope ($2\leq j \leq J$). \cite{sun2012exact} discussed the robustness and sensibility of small values of $J$, and in particular $J=2$ is suggested for large datasets. 
%It is notable that \cite{juul2021fixed}'s sampling-based approach is a special case of functional depth, where $n=500$ and $J=50$ (in their paper: $N_{\rm{curves}}=50$) and random samples were uniformly drawn $N_{\rm{samples}}=100$ times. Likewise the functional depth method, the centrality scores of all curves in the ensemble were updated based on whether the entire curve is contained in the sample-specific envelope. The most central curves are the ones with their scores above the 50\% percentile of scores. Here, we use the functional boxplot on \cite{juul2021fixed}'s dataset, thus ${500}\choose{2}$ samples determine the centrality scores.


% Specifically, we used the function fda() in R \citep{R} package \pkg{roahd} \citep{roahd}, with the choice of modified band depth (MBD) to break ties which is based on the fast algorithm proposed by \cite{sun2012exact}. 
% 

%\bmb{what else do \cite{sun2012exact} say about tradeoffs (other than computational) for using larger values of \ncurve \ldots ? Any clues as to why Juul et al might have chosen their values?}
%\ali{ I think: \\
%1. Functional band depth of a function is sensitive to the shape and the position (or shift) of a function in comparison to the rest of ensemble members. Smaller values for $J$ allow for more shape sensitivity of the approach and are significantly faster to compute \cite{lopez2009concept} (see page 4 of \cite{lopez2009concept} in their example; for $J\geq 3$, The mean integrated error is minimized for $J=3$, thus the deepest curve is the same for $J\geq 3$. Note, mean integrated error is defined as the mean sample error from the sample median curve, $\hat m_{n,J}$ and defined as the curve in the sample with the highest depth value,  / or we can say the changes in sensitivity of the BD, deepest curves, to the choice of $J\geq 3$ is negligible see Fig.3 in \cite{lopez2009concept}).\\
%2. Not sure why Juul et al chose $\ncurve=50$, I have an odd feeling that maybe it is to do with the fact that the peaks lie between day 50 to day 100, in their dataset, and they chose $\ncurve=10$ when they use only the part of curves between day 50 and day 100.  


%We also use pairwise distances between the curves and quantiles of centrality as an alternative method to a sampling-based functional boxplot. In particular, we compute all pairwise distances -- here we used $\ell_2$ norm which gives the area between the two curves -- between the curves, determine the median-like distance of a curve to all others as the minimum of sum of distances, estimate the distribution of distances. The curve-based descriptive statistics is as follows;
%(1) ranking the curves from more central, i.e., closer to the distribution median to less central, and (2) plot the envelope containing the most central curves, i.e. the pointwise min/max curves of a subset of curves with the lowest sum (or sum of squares) of distances to the other curves. Note that the distribution median and mean are highly correlated and in this context median makes the most sense \ali{(the mean of an ensemble is not an appropriate representative of an ensemble \cite{donoho1992breakdown})}. Also, the most central set depends on the choice of the functional norm and further research is needed for clarification. We caution that functional norms that gives a distance of exactly zero on a slightly time-shifted step function (e.g., Fréchet distance and dynamic time-warping \ali{refs?}) seems inappropriate.

%\bmb{took out comment about MD being appropriate only if shape of feature cloud is $\approx$ MVN (in particular not bimodal): add it back in?}
%The envelope of the most central curves includes the curves with the rank within the 50\% quantile. The Mahalanobis distance provides a measure of similarity between multivariate data and uses covariance information between features to weight the contributions to the distance. The Euclidean distance, on the other hand, in essence gives excess weight to variables (features) that are highly correlated and gives additional weight to variables that have similar information. The Mahalanobis distance gives less weight to those variables that have high variance and to those variables that have high correlation, so that other feature variables with lower correlations can contribute to the distance.
%One potential problem with Mahalanobis distances: if (for example) the probe distribution is strongly bimodal, then scaling factors/correlations derived from the overall data set may not be appropriate for scaling the components of distance between two trajectories whose features
% put them in the same mode/component of the distribution.


% \cite{lopez2009concept} comment that ``the band depth order is very stable in $J$''; they use $J=3$

% allowed for ordering a sample of curves from the center outward and, thus, introduced a measure to define functional quantiles and the centrality or outlyingness of an observation. 


\bibliography{../AliMac}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix}


Goal: 
 How to compare Juul's result to ours, quantitative/visual? the point is that in addition to the option that Juul et al gave, here we present other options and you should think about which ones have the properties that you want.
 
What to be presented? 
- maybe 4 panels, here is what you get by various methods. Each panel have several quantile overlaid (50,80,90) 1 Juul's work, 1 Mahalanobis on the probes,  

Notes:
1) The central region (not the curve) is to be focused on. (spend a paragraph) 

2) What did Juul do? Algorithm? 
Out of the whole curves, referred as the whole ensemble, pick a random sensible, consider the pointwise min and max which determines the envelope. $E_{sample}$ is the envelope of the sample, assign a score to each curve in the ensemble, all scores starts with 0 if the curve is not in the envelope, they add $s(c_i)$ but we think the default is constant 1, then you can weight it in some way. 

Juul's concept of centrality goes back to a 2009 paper \cite{lopez2009concept}, recommending of sampling 3 curves at a time (check?). 
- Where Juul's method comes from?
- How depth handles phase variation?
- Search for functional band depth in R, check fbplot in fda package \url{https://www.rdocumentation.org/packages/fda/versions/5.1.9/topics/fbplot} how to specify the size of subensemble.
- check also \cite{sun2012exact}

- This implies that the centrality score cannot be lower than the resampled proportion. 
That is, if the size of a subensemble is $p\%$ of the whole ensemble, any curve in the subensemble will have score of 1 at least $p\%$ of the time but the centrality score cannot be less that $p$. Specifically, each curve in the whole ensemble has a $p\%$ chance to be picked in the subensemble, thus be in the envelope, so the centrality score cannot be less than the size of the subensemble.     
- Is there a upper bound? We are not sure. Thinking about a straight line with other curves are curved up or down etc.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Literature Review}
\citep{juul2021fixed}

\cite{probert2016decision} looking at different metrics (probes) of an epidemic.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{emails}

{\bf Ben,  
date: Feb 9, 2021}

There are several somewhat orthogonal questions, all geared to the general question of "what is a sensible way to define a confidence interval or 'typical' set of curves out of an ensemble?

 1. Do we apply measures of centrality directly to the curves or to
probes derived from the curves?
 2.  What measure of centrality/typical-ness do we apply?
 3.  If it depends on distance, what distance do we use?
 4. Do we define centrality by difference from a central point
(centroid/Fréchet mean) or by average closeness to the rest of the
set? Does the answer differ depending on step 3 (choice of distance)?

(And finally: how do Juul et al's methods fit into this?)

Another email the same day:
you might be able to show that the conjecture does *not* hold for some fairly simple distance metric (e.g. log(Euclidean) or some similar kind of geometric-mean distance), in which case it would also settle most of the question.

\section*{Appendix}

\begin{figure}[ht]\centering
  \includegraphics[width=\linewidth]{scripts/cent_plot2.pdf}
  \caption{Comparison of 90\% central regions computed with
    different functional boxplot and pairwise-distance methods, using epidemic curve ensembles from \juul. FBD = functional band distance; $J$ = number of curves used for centrality calculation. Curve with $J=2$ computed via the \texttt{roahd} package \citep{roahd}, curve with $J=50$ used our own implementation of the FBD algorithm described by \juul, curve with Mahalanobis (5 features) used Mahalanobis distance on features of interest including the peak prevalence, the time at which the peak occurs, the initial growth rate, epidemic duration, and total size of the epidemic.
  }
  \label{p.b}
\end{figure}


\end{document}
